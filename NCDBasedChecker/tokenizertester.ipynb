{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import antlr4\n",
    "from antlr4 import *\n",
    "from LexersAndParsers.Java9 import Java9Lexer,Java9Listener,Java9Parser\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = open(\"./test-files/text3.txt\",\"r\").read()\n",
    "code=\"\"\"/*hellow word\n",
    "*helllo */\"\"\"\n",
    "lexer = Java9Lexer.Java9Lexer(antlr4.InputStream(code))\n",
    "stream = antlr4.CommonTokenStream(lexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=open('./LexersAndParsers/Java9/Java9Lexer.tokens','r').read().split('\\n')\n",
    "number_to_type={}\n",
    "for token in tokens[0:119]:\n",
    "    pair=token.rsplit('=',1)\n",
    "    number_to_type[int(pair[1])]=pair[0]\n",
    "number_to_type[-1]='<EOF>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMMENT'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.getText()\n",
    "number_to_type[stream.tokens[0].type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'CommonToken' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3b486bcf0098>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(i,tokens[2])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(text[tokens[3]:tokens[4]+1])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mnewCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0moldCode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'CommonToken' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "newCode=[]\n",
    "oldCode=[]\n",
    "for i,tokens in enumerate(stream.tokens):\n",
    "    #print(i,tokens[2])\n",
    "    #print(text[tokens[3]:tokens[4]+1])\n",
    "    newCode.append(tokens[2]+\" \")\n",
    "    oldCode.append(tokens[5]+\" \")\n",
    "\n",
    "print (len(code))\n",
    "print(\"\".join(newCode))   \n",
    "print(\"\".join(oldCode))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = open(\"./test-files/text3.txt\",\"r\").read()\n",
    "lexer = Java9Lexer.Java9Lexer(antlr4.InputStream(code))\n",
    "stream = antlr4.CommonTokenStream(lexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream.tokens[8].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import JavaTokenizer\n",
    "toke= JavaTokenizer(\"./test-files/text3.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'T__0',\n",
       " 2: 'T__1',\n",
       " 3: 'T__2',\n",
       " 4: 'T__3',\n",
       " 5: 'T__4',\n",
       " 6: 'T__5',\n",
       " 7: 'T__6',\n",
       " 8: 'T__7',\n",
       " 9: 'T__8',\n",
       " 10: 'T__9',\n",
       " 11: 'ABSTRACT',\n",
       " 12: 'ASSERT',\n",
       " 13: 'BOOLEAN',\n",
       " 14: 'BREAK',\n",
       " 15: 'BYTE',\n",
       " 16: 'CASE',\n",
       " 17: 'CATCH',\n",
       " 18: 'CHAR',\n",
       " 19: 'CLASS',\n",
       " 20: 'CONST',\n",
       " 21: 'CONTINUE',\n",
       " 22: 'DEFAULT',\n",
       " 23: 'DO',\n",
       " 24: 'DOUBLE',\n",
       " 25: 'ELSE',\n",
       " 26: 'ENUM',\n",
       " 27: 'EXTENDS',\n",
       " 28: 'FINAL',\n",
       " 29: 'FINALLY',\n",
       " 30: 'FLOAT',\n",
       " 31: 'FOR',\n",
       " 32: 'IF',\n",
       " 33: 'GOTO',\n",
       " 34: 'IMPLEMENTS',\n",
       " 35: 'IMPORT',\n",
       " 36: 'INSTANCEOF',\n",
       " 37: 'INT',\n",
       " 38: 'INTERFACE',\n",
       " 39: 'LONG',\n",
       " 40: 'NATIVE',\n",
       " 41: 'NEW',\n",
       " 42: 'PACKAGE',\n",
       " 43: 'PRIVATE',\n",
       " 44: 'PROTECTED',\n",
       " 45: 'PUBLIC',\n",
       " 46: 'RETURN',\n",
       " 47: 'SHORT',\n",
       " 48: 'STATIC',\n",
       " 49: 'STRICTFP',\n",
       " 50: 'SUPER',\n",
       " 51: 'SWITCH',\n",
       " 52: 'SYNCHRONIZED',\n",
       " 53: 'THIS',\n",
       " 54: 'THROW',\n",
       " 55: 'THROWS',\n",
       " 56: 'TRANSIENT',\n",
       " 57: 'TRY',\n",
       " 58: 'VOID',\n",
       " 59: 'VOLATILE',\n",
       " 60: 'WHILE',\n",
       " 61: 'UNDER_SCORE',\n",
       " 62: 'IntegerLiteral',\n",
       " 63: 'FloatingPointLiteral',\n",
       " 64: 'BooleanLiteral',\n",
       " 65: 'CharacterLiteral',\n",
       " 66: 'StringLiteral',\n",
       " 67: 'NullLiteral',\n",
       " 68: 'LPAREN',\n",
       " 69: 'RPAREN',\n",
       " 70: 'LBRACE',\n",
       " 71: 'RBRACE',\n",
       " 72: 'LBRACK',\n",
       " 73: 'RBRACK',\n",
       " 74: 'SEMI',\n",
       " 75: 'COMMA',\n",
       " 76: 'DOT',\n",
       " 77: 'ELLIPSIS',\n",
       " 78: 'AT',\n",
       " 79: 'COLONCOLON',\n",
       " 80: 'ASSIGN',\n",
       " 81: 'GT',\n",
       " 82: 'LT',\n",
       " 83: 'BANG',\n",
       " 84: 'TILDE',\n",
       " 85: 'QUESTION',\n",
       " 86: 'COLON',\n",
       " 87: 'ARROW',\n",
       " 88: 'EQUAL',\n",
       " 89: 'LE',\n",
       " 90: 'GE',\n",
       " 91: 'NOTEQUAL',\n",
       " 92: 'AND',\n",
       " 93: 'OR',\n",
       " 94: 'INC',\n",
       " 95: 'DEC',\n",
       " 96: 'ADD',\n",
       " 97: 'SUB',\n",
       " 98: 'MUL',\n",
       " 99: 'DIV',\n",
       " 100: 'BITAND',\n",
       " 101: 'BITOR',\n",
       " 102: 'CARET',\n",
       " 103: 'MOD',\n",
       " 104: 'ADD_ASSIGN',\n",
       " 105: 'SUB_ASSIGN',\n",
       " 106: 'MUL_ASSIGN',\n",
       " 107: 'DIV_ASSIGN',\n",
       " 108: 'AND_ASSIGN',\n",
       " 109: 'OR_ASSIGN',\n",
       " 110: 'XOR_ASSIGN',\n",
       " 111: 'MOD_ASSIGN',\n",
       " 112: 'LSHIFT_ASSIGN',\n",
       " 113: 'RSHIFT_ASSIGN',\n",
       " 114: 'URSHIFT_ASSIGN',\n",
       " 115: 'Identifier',\n",
       " 116: 'WS',\n",
       " 117: 'COMMENT',\n",
       " 118: 'LINE_COMMENT',\n",
       " 119: 'UNK',\n",
       " -1: 'EOF',\n",
       " 0: 'PAD'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toke.generateDict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = open('./test-files/text4.txt', 'r').read()\n",
    "lexer = Java9Lexer.Java9Lexer(antlr4.InputStream(code))\n",
    "stream = antlr4.CommonTokenStream(lexer)\n",
    "parser= Java9Parser.Java9Parser(stream)\n",
    "tree=parser.compilationUnit()\n",
    "#print(tree.toStringTree(recog=parser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"ENCODING def count_word LPAR s RPAR COLON INDENT s EQUAL NUMBER h EQUAL NUMBER PLUS s for f in range LPAR len LPAR s RPAR RPAR COLON INDENT print LPAR s LSQB f RSQB RPAR DEDENT return len LPAR s DOT split LPAR ' ' RPAR RPAR DEDENT ENDMARKER \", \"utf-8 def count_word ( s ) :      s = 1 h = 1 + s for f in range ( len ( s ) ) :          print ( s [ f ] )  return len ( s . split ( ' ' ) )   \")\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import PythonTokenizer\n",
    "import token\n",
    "\n",
    "\n",
    "code= \"\"\"def count_word(s):\n",
    "    basd=1\n",
    "    aas=1+basd\n",
    "    for isa in range(len(s)):\n",
    "        print(s[isa])\n",
    "    return len(s.split(' '))\"\"\"\n",
    "tok=PythonTokenizer(None,code,name_generator)\n",
    "print(tok.getTokenizedCode(obfuscate_var=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyminifier import obfuscate\n",
    "import tokenize\n",
    "from io import BytesIO\n",
    "code=\"\"\"def word_count(text):\n",
    "    num_words =0\n",
    "    num_words = 1 + text.count(' ')\n",
    "    text[num_words]\n",
    "    return num_words\n",
    "\"\"\"\n",
    "tokens =tokenize.tokenize(BytesIO(code.encode()).readline)\n",
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    _delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_generator = obfuscate.obfuscation_machine(False, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'def', (1, 0), (1, 3), 'def count_word(s):\\n'], [1, 'count_word', (1, 4), (1, 14), 'def count_word(s):\\n'], [54, '(', (1, 14), (1, 15), 'def count_word(s):\\n'], [1, 's', (1, 15), (1, 16), 'def count_word(s):\\n'], [54, ')', (1, 16), (1, 17), 'def count_word(s):\\n'], [54, ':', (1, 17), (1, 18), 'def count_word(s):\\n'], [4, '\\n', (1, 18), (1, 19), 'def count_word(s):\\n'], [5, '    ', (2, 0), (2, 4), '    basd=1\\n'], [1, 'basd', (2, 4), (2, 8), '    basd=1\\n'], [54, '=', (2, 8), (2, 9), '    basd=1\\n'], [2, '1', (2, 9), (2, 10), '    basd=1\\n'], [4, '\\n', (2, 10), (2, 11), '    basd=1\\n'], [1, 'aas', (3, 4), (3, 7), '    aas=1+basd\\n'], [54, '=', (3, 7), (3, 8), '    aas=1+basd\\n'], [2, '1', (3, 8), (3, 9), '    aas=1+basd\\n'], [54, '+', (3, 9), (3, 10), '    aas=1+basd\\n'], [1, 'basd', (3, 10), (3, 14), '    aas=1+basd\\n'], [4, '\\n', (3, 14), (3, 15), '    aas=1+basd\\n'], [1, 'for', (4, 4), (4, 7), '    for isa in range(len(s)):\\n'], [1, 'isa', (4, 8), (4, 11), '    for isa in range(len(s)):\\n'], [1, 'in', (4, 12), (4, 14), '    for isa in range(len(s)):\\n'], [1, 'range', (4, 15), (4, 20), '    for isa in range(len(s)):\\n'], [54, '(', (4, 20), (4, 21), '    for isa in range(len(s)):\\n'], [1, 'len', (4, 21), (4, 24), '    for isa in range(len(s)):\\n'], [54, '(', (4, 24), (4, 25), '    for isa in range(len(s)):\\n'], [1, 's', (4, 25), (4, 26), '    for isa in range(len(s)):\\n'], [54, ')', (4, 26), (4, 27), '    for isa in range(len(s)):\\n'], [54, ')', (4, 27), (4, 28), '    for isa in range(len(s)):\\n'], [54, ':', (4, 28), (4, 29), '    for isa in range(len(s)):\\n'], [4, '\\n', (4, 29), (4, 30), '    for isa in range(len(s)):\\n'], [5, '        ', (5, 0), (5, 8), '        print(s[isa])\\n'], [1, 'print', (5, 8), (5, 13), '        print(s[isa])\\n'], [54, '(', (5, 13), (5, 14), '        print(s[isa])\\n'], [1, 's', (5, 14), (5, 15), '        print(s[isa])\\n'], [54, '[', (5, 15), (5, 16), '        print(s[isa])\\n'], [1, 'isa', (5, 16), (5, 19), '        print(s[isa])\\n'], [54, ']', (5, 19), (5, 20), '        print(s[isa])\\n'], [54, ')', (5, 20), (5, 21), '        print(s[isa])\\n'], [4, '\\n', (5, 21), (5, 22), '        print(s[isa])\\n'], [6, '', (6, 4), (6, 4), \"    return len(s.split(' '))\"], [1, 'return', (6, 4), (6, 10), \"    return len(s.split(' '))\"], [1, 'len', (6, 11), (6, 14), \"    return len(s.split(' '))\"], [54, '(', (6, 14), (6, 15), \"    return len(s.split(' '))\"], [1, 's', (6, 15), (6, 16), \"    return len(s.split(' '))\"], [54, '.', (6, 16), (6, 17), \"    return len(s.split(' '))\"], [1, 'split', (6, 17), (6, 22), \"    return len(s.split(' '))\"], [54, '(', (6, 22), (6, 23), \"    return len(s.split(' '))\"], [3, \"' '\", (6, 23), (6, 26), \"    return len(s.split(' '))\"], [54, ')', (6, 26), (6, 27), \"    return len(s.split(' '))\"], [54, ')', (6, 27), (6, 28), \"    return len(s.split(' '))\"], [4, '', (6, 28), (6, 29), ''], [6, '', (7, 0), (7, 0), ''], [0, '', (7, 0), (7, 0), '']]\n"
     ]
    }
   ],
   "source": [
    "options= dotdict()\n",
    "options.obf_variables =True\n",
    "options.obfuscate=False\n",
    "options.replacement_length=1\n",
    "toks=token_utils.listified_tokenizer(code)\n",
    "\n",
    "print(toks)\n",
    "obfuscate.obfuscate(module=\"Q2C116\",tokens=toks , options=options, name_generator = name_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def count_word(s):\\n    a=1\\n    U=1+a\\n    for P in range(len(s)):\\n        print(s[P])\\n    return len(s.split(' '))\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_utils.untokenize(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def count_word(s):\\n    a=1\\n    U=1+a\\n    for P in range(len(s)):\\n        print(s[P])\\n    return len(s.split(' '))\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_utils.untokenize(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyminifier import analyze\n",
    "from pyminifier import token_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_x=124\n",
      "z_y=72\n",
      "z_xy=124\n",
      "s1,s2: 0.41935483870967744\n",
      "z_x=124\n",
      "z_y=72\n",
      "z_xy=124\n",
      "s1,s3: 0.41935483870967744\n",
      "z_x=72\n",
      "z_y=72\n",
      "z_xy=80\n",
      "s2,s3: 0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "from NCD import NCD\n",
    "import random\n",
    "s1=''.join(['b' if k==1 else 'a' for k in [ random.randint(0,1) for i in range(200)]])\n",
    "s2 = ''.join(['ab']*100)\n",
    "s3 = ''.join(['ba']*100)\n",
    "#tok=PythonTokenizer(None,code_085,name_generator)\n",
    "#code_085_tok= tok.getTokenizedCode(obfuscate_var=True)[0]\n",
    "#tok=PythonTokenizer(None,code_080,name_generator)\n",
    "#code_080_tok= tok.getTokenizedCode(obfuscate_var=True)[0]\n",
    "ncd=NCD(s1.encode(),s2.encode())\n",
    "print(\"s1,s2:\",ncd.similarityCheck())\n",
    "ncd=NCD(s1.encode(),s3.encode())\n",
    "print(\"s1,s3:\",ncd.similarityCheck())\n",
    "ncd=NCD(s2.encode(),s3.encode())\n",
    "print(\"s2,s3:\",ncd.similarityCheck())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bcc70dfcff74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlzma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_080\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mlzma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode_080\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "import lzma\n",
    "print((code_080,\"utf8\").encode())\n",
    "lzma.compress(code_080.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
